{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "## Outline:\n",
    "1. Transfer Learning\n",
    "1. ELMO  \n",
    "1. BERT  \n",
    "1. GPT-2 \n",
    "1. Natural Language Understanding\n",
    "1. transformers lib\n",
    "\n",
    "\n",
    "## Readings\n",
    "1. https://jalammar.github.io/illustrated-gpt2/  \n",
    "1. http://jalammar.github.io/illustrated-bert/\n",
    "1. https://gluebenchmark.com/\n",
    "1. https://medium.com/modern-nlp/transfer-learning-in-nlp-f5035cc3f62f  \n",
    "1. https://lena-voita.github.io/posts/emnlp19_evolution.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Transfer Learning\n",
    "\n",
    "Why transfer learning? https://arxiv.org/pdf/2001.08361.pdf\n",
    "\n",
    "<img src=\"images/trans.png\" style=\"height:300px\">\n",
    "\n",
    "Types of transfer learning (subset)\n",
    "1. Domain adaptation: Representational approach. Try changing the underlying distribution of data by either finding features that are common in both domain or represent both data in a shared low-dimensional space\n",
    "1. Multitask Learning\n",
    "1. ...\n",
    "\n",
    "Differences between source and target domains:\n",
    "1. The feature space of source and target is different.\n",
    "1. The marginal probability distribution of words is different for source and target.\n",
    "1. Labels differ for source and target.\n",
    "1. The marginal probability distribution of labels is different for source and target.\n",
    "1. The condition probability distribution of labels is different.\n",
    "\n",
    "### MultiTask Learning\n",
    "\n",
    "<img src=\"images/multi1.png\" style=\"height:300px\">\n",
    "\n",
    "Why?  \n",
    "1. Implicit data augmentation: Effectively, MTL increases the training data for our model.\n",
    "1. Implicit feature selection. Training on multiple task can teach the model to focus on the most relevant features and can lead to a better model.\n",
    "1. Representation bias: MTL forces the model to learn representations which are useful for all tasks. This helps the model to generalize faster for all tasks in the future as the representation which works for many tasks will also work for a new one.\n",
    "1. Regularization: MTL acts as a regularizer by introducing inductive bias and reduces Rademacher complexity of the model.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "! Notion of catastrophic forgetting = direct finetuning of pretrained model on your downstream task can result in loss of information from pretrained data.\n",
    "\n",
    "Downstream task = task to finetune on\n",
    "\n",
    "Optimization Schemes:\n",
    "1. feature extraction = do not change the pretrained weights \n",
    "1. fine-tuning = change the pretrained weights\n",
    "    1. Progressively in time (freezing) \n",
    "    1. Progressively in intensity (lower learning rates)\n",
    "    1. Progressively vs. a pretrained model (regularization)\n",
    "    \n",
    "    \n",
    "Evolution of BERT layer representations from https://arxiv.org/abs/1909.01380\n",
    "1. with the language model objective, as you go from bottom to top layers, information about the past gets lost and predictions about the future get formed;\n",
    "1. for masked language model, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation; the token identity then gets recreated at the top layer;\n",
    "1. for machine translation, though representations get refined with context, less processing is happening and most information about the word type does not get lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 ELMO\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/1802.05365\n",
    "\n",
    "**Training objective**: Language Modeling  (forward and backward)\n",
    "\n",
    "**Downstream tasks**:\n",
    "\n",
    "Embedding of k-th token for specific task\n",
    "$$ ELMO^{task}_k = \\gamma^{task} \\sum_{j=0}^L s^{task} h_{k,j}^{LM}$$\n",
    "where  \n",
    "$k$ - k-th token  \n",
    "$s^{task}$ - learnable softmax-normalized weights  \n",
    "$\\gamma^{task}$ - learnable scaling parameter  \n",
    "\n",
    "<img src=\"images/elmo.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "# Compute two different representation for each token.\n",
    "# Each representation is a linear weighted combination for the\n",
    "# 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)\n",
    "\n",
    "# use batch_to_ids to convert sentences to character ids\n",
    "sentences = [['First', 'sentence', '.', 'custom', 'service'], ['Another', '.']]\n",
    "character_ids = batch_to_ids(sentences)\n",
    "\n",
    "embeddings = elmo(character_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 BERT\n",
    "\n",
    "Bert consists only from encoder layers.\n",
    "\n",
    "<img src=\"images/bert.png\" style=\"height:300px\">\n",
    "\n",
    "\n",
    "Training objectives:\n",
    "1. Masked Language Model\n",
    "2. Sentence Entailment\n",
    "\n",
    "<img src=\"images/bert_training.png\" style=\"height:300px\">\n",
    "\n",
    "\n",
    "Classification with BERT\n",
    "<img src=\"images/bert_cls.png\" style=\"height:300px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2742, 1035, 3793,  102]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = 'example_text'\n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)]) \n",
    "print(input_ids)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 GPT-2\n",
    "\n",
    "GPT-2 consists only from decoder blocks.  \n",
    "Training objective: Language Model  \n",
    "\n",
    "<img src=\"images/gpt.png\" style=\"height:300px\">\n",
    "\n",
    "Downstream Tasks:\n",
    "Essentially, you convert all downstream tasks to satisfy language model objective.\n",
    "\n",
    "<img src=\"images/gpt2.png\" style=\"height:300px\">\n",
    "\n",
    "<img src=\"images/gpt3.png\" style=\"height:300px\">\n",
    "\n",
    "e.g. Machine Translation \n",
    "\n",
    "<img src=\"images/gpt4.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Natural Language Understanding\n",
    "\n",
    "Most modern models are evaluated on downstream tasks (aka transfer learning), despite their training objective.\n",
    "\n",
    "<img src=\"images/nlu.jpeg\" style=\"height:300px\">\n",
    "\n",
    "\"The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
