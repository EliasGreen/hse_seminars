{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9\n",
    "\n",
    "Use data from `https://github.com/thedenaas/hse_seminars/tree/master/2018/seminar_13/data.zip`  \n",
    "Implement model in pytorch from \"An Unsupervised Neural Attention Model for Aspect Extraction, He et al, 2017\", also desribed in seminar notes.  \n",
    "\n",
    "You can use sentence embeddings with attention **[7 points]**:  \n",
    "$z_s = \\sum_{i}^n \\alpha_i e_{w_i}, z_s \\in R^d$ sentence embedding  \n",
    "$\\alpha_i = softmax(d_i)$  attention weight for i-th token  \n",
    "$d_i = e_{w_i}^T M y_s$ attention with trainable matrix $M \\in R^{dxd}$  \n",
    "$y_s = \\frac 1 n \\sum_{i=1}^n e_{w_i}, y_s \\in R^d$ sentence context  \n",
    "$e_{w_i} \\in R^d$, token embedding of size d  \n",
    "$n$ - number of tokens in a sentence  \n",
    "\n",
    "**Or** just use sentence embedding as an average over word embeddings **[5 points]**:  \n",
    "$z_s = \\frac 1 n \\sum_{i=1}^n e_{w_i}, z_s \\in R^d$ sentence embedding  \n",
    "$e_{w_i} \\in R^d$, token embedding of size d  \n",
    "$n$ - number of tokens in a sentence  \n",
    " \n",
    "$p_t = softmax(W z_s + b), p_t \\in R^K$ topic weights for sentence $s$, with trainable matrix $W \\in R^{dxK}$ and bias vector $b \\in R^K$  \n",
    "$r_s = T^T p_t, r_s \\in R^d$ reconstructed sentence embedding as a weighted sum of topic embeddings   \n",
    "$T \\in R^{Kxd}$ trainable matrix of topic embeddings, K=number of topics\n",
    "\n",
    "\n",
    "**Training objective**:\n",
    "$$ J = \\sum_{s \\in D} \\sum_{i=1}^n max(0, 1-r_s^T z_s + r_s^T n_i) + \\lambda ||T^T T - I ||^2_F  $$\n",
    "where   \n",
    "$m$ random sentences are sampled as negative examples from dataset $D$ for each sentence $s$  \n",
    "$n_i = \\frac 1 n \\sum_{i=j}^n e_{w_j}$ average of word embeddings in the i-th sentence  \n",
    "$||T^T T - I ||_F$ regularizer, that enforces matrix $T$ to be orthogonal  \n",
    "$||A||^2_F = \\sum_{i=1}^N\\sum_{j=1}^M a_{ij}^2, A \\in R^{NxM}$ Frobenius norm\n",
    "\n",
    "\n",
    "**[3 points]** Compute topic coherence for at least for 3 different number of topics. Use 10 nearest words for each topic. It means you have to train one model for each number of topics. You can use code from seminar notes with word2vec similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
