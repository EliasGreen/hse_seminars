{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed representations\n",
    "\n",
    "1. Word2vec  \n",
    "    1.1 skip-gram model  \n",
    "    1.2 Hierarchical Huffman Trees  \n",
    "    1.3 Negative Sampling  \n",
    "    1.4 CBoW model   \n",
    "2. Glove  \n",
    "3. FastText  \n",
    "4. Context-dependent Embeddings (e.g. BERT)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings\n",
    "1. (general 1) https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "1. (general 2) https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795\n",
    "1. (glove ) https://towardsdatascience.com/emnlp-what-is-glove-part-i-3b6ce6a7f970\n",
    "1. (embeddings in pytorch) https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "1. http://jalammar.github.io/illustrated-bert/\n",
    "1. https://arxiv.org/abs/1802.05365"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Word2vec\n",
    "<img src=\"images/w2v.png\" style=\"height:300px\">\n",
    "\n",
    "## 1.1 Skip-gram model\n",
    "<img src=\"images/skip.png\" style=\"height:500px\">\n",
    "\n",
    "For each word $t$ predict surrounding words in a windox of size $m$ (context)\n",
    "\n",
    "Objective is to maximize probability of context words given the current center word:  \n",
    "    \n",
    "$$J(\\theta) = \\prod^T_{t=1} \\prod_{-m \\le j \\le m; j != 0 }  p(x_{t+j} | x_t; \\theta)  \\rightarrow max $$,\n",
    "where  \n",
    "$x_t$ - center word,  \n",
    "$x_{t+j}$ - word from context,  \n",
    "$m$ - context size.  \n",
    "\n",
    "or negative log-likelihood:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum^T_{t=1} \\sum_{-m \\le j \\le m; j != 0 }  log p(x_{t+j} | x_t; \\theta)  \\rightarrow min $$\n",
    "\n",
    "$$p(x_{t+j} | x_t) = p(out | center) = \\frac{\\exp(u_{out}^T v_{center})}{\\sum_{k=1}^K \\exp(u_{k}^T v_{center})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Hierarchial Huffman trees\n",
    "\n",
    "Complexity $O(V) \\rightarrow O(\\log_2 V)$\n",
    "\n",
    "$x = v_{n(w,j)}^T v_{w}$,   \n",
    "where $n(w,j)$ is the j-th node on the path from the root to $w$.  \n",
    "\n",
    "$p(n, left) = \\sigma (v_n^T v_w)$ - probability to go left.  \n",
    "$p(n, right) = \\sigma (- v_n^T v_w )$ - probability to go right.  \n",
    "\n",
    "Then,  \n",
    "$p(w_j | w) = \\prod_{j=1}^{L(w) - 1} \\sigma ( [ n(w, j+1) == child(n(w,j)) ] v_n^T v_w)$,  \n",
    "where $L(w)$ - depth of the tree,  \n",
    "$child(n)$ - child of node n.\n",
    "\n",
    "\n",
    "<img src=\"images/hier.png\" style=\"height:200px\">\n",
    "\n",
    "How to build binary prefix tree? -> Huffman Tree.\n",
    "<img src=\"images/huffman.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Negative sampling\n",
    "\n",
    "Using negative sampling with k samples:   \n",
    "    \n",
    "$\\log p(w_{t+j} | w_t; \\theta) = \\log \\sigma(u_{outer}^T v_{center})  + \\sum_{i=1}^k E_{j \\sim P(w)} [\\log \\sigma (-u_j^T v_{center})]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = df.text.apply(lambda x: x.split()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.73 s, sys: 13.4 ms, total: 3.75 s\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "w2v = Word2Vec(sentences, negative=5, size=100, iter=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('airline.', 0.9007248878479004),\n",
       " ('best', 0.8687483072280884),\n",
       " ('ever', 0.8615865111351013),\n",
       " ('awful', 0.8586821556091309),\n",
       " ('most', 0.8517616987228394),\n",
       " ('worst', 0.84912109375),\n",
       " ('disappointed', 0.8412606716156006),\n",
       " ('horrible', 0.838492751121521),\n",
       " ('company', 0.8372955322265625),\n",
       " ('absolute', 0.8371437788009644)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('airline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 CBOW model\n",
    "\n",
    "= Predict center word from surrounding context\n",
    "\n",
    "<img src=\"images/cbow.png\" style=\"height:400px\">\n",
    "\n",
    "$$h = W^T x$$  \n",
    "$$x = [x_{j-m}, x_{j-m+1}, ... x_{j-1}, x_{j+1}, ..., x_{j+m}] $$  \n",
    "\n",
    "$$p(x_j | x) = \\frac{\\exp(v_j^T h)}{\\sum_{k=1}^K \\exp(v_k^T h)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "w2v = Word2Vec(sentences, negative=5, size=100, iter=100, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('assult', 0.6154369115829468),\n",
       " ('reported', 0.45402228832244873),\n",
       " ('most', 0.4082372188568115),\n",
       " ('communication,', 0.40658941864967346),\n",
       " ('Delays', 0.38628947734832764),\n",
       " ('Gate', 0.38524293899536133),\n",
       " ('Atlantic', 0.38465529680252075),\n",
       " ('engine', 0.3818504810333252),\n",
       " ('computer', 0.3697021007537842),\n",
       " ('SNA', 0.35934001207351685)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar('police')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Glove\n",
    "\n",
    "= Word embeddings through decomposition of co-occurance matrix\n",
    "\n",
    "<img src=\"images/matrix.png\" style=\"height:300px\">\n",
    "\n",
    "$P_{ij}$ - occurance of i-th word along with j-th in the window of size m\n",
    "\n",
    "Cons: \n",
    "1. Very high-dimensional, not used in practice\n",
    "2. Hard to add new words and docs\n",
    "\n",
    "Trivial solution: use some dimension-reduction method, usually SVD\n",
    "\n",
    "Singular Value Decomposition\n",
    "\n",
    "$M = U \\Sigma V$  \n",
    "$Mv = \\sigma u$  \n",
    "$M^{*}u = \\sigma v$   \n",
    "U, V are unitary matrices  \n",
    "$\\Sigma$ - diagonal\n",
    "\n",
    "\n",
    "$O(nm^2)$ for case n < m\n",
    "\n",
    "\n",
    "<img src=\"images/glove.png\" style=\"height:300px\">\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2} \\sum_{i,j=1}^W f(P_{ij})(u_i^T v_j - log P_{ij})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 FastText\n",
    "\n",
    "Subword embeddings.\n",
    "\n",
    "Introduce **scoring function** (instead of scalar product in w2v):\n",
    "$$s(w,c) = \\sum_{g \\in G_w} z_g^T v_c$$\n",
    "where  \n",
    "$G_w$ - set of 3-grams appearing in word $w$  \n",
    "$z_g$ - embedding of 3-gram g  \n",
    "$v_c$ - context vector  \n",
    "\n",
    "\n",
    "**Objective** function for skip-gram case:\n",
    "\n",
    "$$  \\sum_{t=1}^T [\\sum_{c \\in C_t}log (1 + \\exp(- s(w_t, w_c))) + \\sum_{n \\in N_{t,c}} \\log(1 + \\exp(s(w_t, n)))] \\rightarrow \\min$$\n",
    "\n",
    "where  \n",
    "$c$ - chosen context position  \n",
    "$C_t$ set of context position dependent on current word $t$  \n",
    "$T$ - total number of words  \n",
    "$N_{t,c}$ - set of negative samples dependent on chosen word and context  \n",
    "\n",
    "\n",
    "**Inference**:\n",
    "Embedding of word $w$ from 3-grams $G_w$:\n",
    "$$v_w = \\sum_{g \\in G_w} z_g$$\n",
    "\n",
    "\n",
    "\n",
    "**Tweaks** in Negative sampling: sampling negative examples with probability \n",
    "$$ p(w) = \\frac {\\sqrt {U(w)}} {Z}$$\n",
    "where $Z = \\sum_w \\sqrt {U(w)}$  \n",
    "and $U(w)$ - the count of a particular word $w$  \n",
    "\n",
    "Probability of token $w$ to be discarded during training:\n",
    "$$ P(w) = \\sqrt {\\frac t {f(w)}} + \\frac t {f(w)} $$\n",
    "where $f(w) = \\frac {U(w)} Z$ - frequency of token $w$  \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/ft.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Context-dependent Embeddings (e.g. BERT)\n",
    "\n",
    "Embedding of token depends on the context.\n",
    "\n",
    "<img src=\"images/bert.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis.litvinov/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/denis.litvinov/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/denis.litvinov/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/denis.litvinov/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/denis.litvinov/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/denis.litvinov/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "I1211 18:04:43.223597 139916672685888 file_utils.py:32] TensorFlow version 2.0.0 available.\n",
      "I1211 18:04:43.224047 139916672685888 file_utils.py:39] PyTorch version 1.3.0 available.\n",
      "I1211 18:04:43.402624 139916672685888 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "I1211 18:04:44.165966 139916672685888 tokenization_utils.py:373] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/denis.litvinov/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'like', 'to', 'drive', 'a', 'car', '[SEP]']\n",
      "['[CLS]', 'i', 'had', 'a', 'ticket', 'for', '8th', 'train', 'car', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_1 = \"I like to drive a car\"\n",
    "text_2 = \"I had a ticket for 8th train car\"\n",
    "marked_text_1 = \"[CLS] \" + text_1 + \" [SEP]\"\n",
    "marked_text_2 = \"[CLS] \" + text_2 + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text_1 = tokenizer.tokenize(marked_text_1)\n",
    "tokenized_text_2 = tokenizer.tokenize(marked_text_2)\n",
    "\n",
    "print(tokenized_text_1)\n",
    "print(tokenized_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_tokens_1 = tokenizer.convert_tokens_to_ids(tokenized_text_1)\n",
    "segments_ids_1 = [1] * len(tokenized_text_1)\n",
    "indexed_tokens_2 = tokenizer.convert_tokens_to_ids(tokenized_text_2)\n",
    "segments_ids_2 = [1] * len(tokenized_text_2)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor_1 = torch.tensor([indexed_tokens_1])\n",
    "segments_tensors_1 = torch.tensor([segments_ids_1])\n",
    "tokens_tensor_2 = torch.tensor([indexed_tokens_2])\n",
    "segments_tensors_2 = torch.tensor([segments_ids_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1211 18:06:03.633340 139916672685888 file_utils.py:296] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpmut8pmm2\n",
      "100%|██████████| 313/313 [00:00<00:00, 436441.87B/s]\n",
      "I1211 18:06:04.196304 139916672685888 file_utils.py:309] copying /tmp/tmpmut8pmm2 to cache at /home/denis.litvinov/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1211 18:06:04.199301 139916672685888 file_utils.py:313] creating metadata file for /home/denis.litvinov/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1211 18:06:04.199918 139916672685888 file_utils.py:322] removing temp file /tmp/tmpmut8pmm2\n",
      "I1211 18:06:04.200409 139916672685888 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/denis.litvinov/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "I1211 18:06:04.200976 139916672685888 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1211 18:06:04.754622 139916672685888 file_utils.py:296] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpxxbr_3re\n",
      "100%|██████████| 440473133/440473133 [01:06<00:00, 6603013.83B/s] \n",
      "I1211 18:07:12.014802 139916672685888 file_utils.py:309] copying /tmp/tmpxxbr_3re to cache at /home/denis.litvinov/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I1211 18:07:12.257982 139916672685888 file_utils.py:313] creating metadata file for /home/denis.litvinov/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I1211 18:07:12.258581 139916672685888 file_utils.py:322] removing temp file /tmp/tmpxxbr_3re\n",
      "I1211 18:07:12.288536 139916672685888 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/denis.litvinov/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers_1, _ = model(tokens_tensor_1, segments_tensors_1)\n",
    "    encoded_layers_2, _ = model(tokens_tensor_2, segments_tensors_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_layers_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(158.6157)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_embedding_1 = encoded_layers_1[0, tokenized_text_1.index('car'), :]\n",
    "car_embedding_2 = encoded_layers_2[0, tokenized_text_2.index('car'), :]\n",
    "\n",
    "torch.sum((car_embedding_1 - car_embedding_2)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
