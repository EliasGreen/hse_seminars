{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models for classifications\n",
    "\n",
    "Outline:\n",
    "\n",
    "1. Tf-idf features\n",
    "1. Encoding categorical features\n",
    "1. Logistic Regression\n",
    "1. Multiclass classification\n",
    "1. SVM\n",
    "1. Kernels\n",
    "1. Evaluation\n",
    "\n",
    "Readings:\n",
    "\n",
    "1. Boshop. Pattern Recognition and Machine Learning. pp 178-220\n",
    "1. Sokolov  lectures on Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Tf-Idf features\n",
    "\n",
    "$$x_{ij} = tf_{ij} idf_j$$\n",
    "where $tf_{ij}$ is frequency of term $j$ in the sample $i$  \n",
    "$$idf_j = \\log (\\frac n {df_j + 1}) + 1$$  \n",
    "$df_j$ - number of samples, where term $j$ present  \n",
    "$n$ - total number of samples  \n",
    "\n",
    "Usually we also apply normalization to discount sentence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Encoding categorical features\n",
    "\n",
    "1. One-hot enocoding\n",
    "\n",
    "<img src=\"images/onehot.png\" style=\"height:200px\">\n",
    "\n",
    "1. Label encoding \n",
    "\n",
    "<img src=\"images/label.webp\" style=\"height:200px\">\n",
    "\n",
    "1. Target-mean encoding\n",
    "\n",
    "<img src=\"images/target_mean.png\" style=\"height:200px\">\n",
    "\n",
    "1. Embeddings (in neural nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Logistic regression \n",
    "\n",
    "Suppose $\\hat y = f(x)$ and $y \\in \\{0, 1\\}$, Show, that $f(x_i)$ should be $p(y=1 | x_i)$.   \n",
    "Probability to generate such samples from the point view of $f(x)$:\n",
    "\n",
    "$$likelihood = \\prod_{i=1}^N f(x_i)^{[y_i=1]} (1 - f(x_i))^{[y_i=0]} \\rightarrow \\max_{f}$$\n",
    "$$log likelihood = \\sum_{i=1}^N [y_i=1] \\log f(x_i) + [y_i=0](1 - f(x_i)) \\rightarrow \\max_{f}$$\n",
    "$$ L = - \\frac 1 N \\sum_{i=1}^N [y_i=1] \\log f(x_i) + [y_i=0](1 - f(x_i)) \\rightarrow \\min_{f}$$\n",
    "$$ E[L|x] = E[- \\frac 1 N \\sum_{i=1}^N [y_i=1] \\log f(x_i) + [y_i=0](1 - f(x_i))]  = \\\\\n",
    "    -p(y=1 |x) \\log f(x) - p(y=0 |x)(1 - f(x)) \\rightarrow \\max_{f} $$    \n",
    "$$ \\frac {\\partial E[L|x]} {\\partial f} = - \\frac {p(y=1|x)}{f(x)} + \\frac {1 - p(y=0 |x)} {1 - f(x)} = 0$$\n",
    "\n",
    "from which we conclude\n",
    "    $$f(x) = p(y=1|x)$$\n",
    "\n",
    "\n",
    "You can find 2 different formulae for logistic loss  \n",
    "First, with margins \n",
    "$$ L(y_i,x_i) = \\log (1 + e^{-y_i w^T x_i})$$\n",
    "\n",
    "<img src=\"images/logmargin.png\" style=\"height:300px\">\n",
    "\n",
    "Second, aka cross-entropy \n",
    "$$ L(y_i, p_i) = - y_i \\log p_i - (1-y_i) \\log(1 - p_i) $$\n",
    "\n",
    "\n",
    "Decision function = probability of $y=1$ class given sample $x$\n",
    "$$ p(y=1 | x_i) = \\sigma(x_i) = \\frac 1 {1 + e^{-w^Tx_i}}$$\n",
    "\n",
    "<img src=\"images/sigmoid.png\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Multiclass classification\n",
    "\n",
    "### Multinomial\n",
    "\n",
    "Cross-entropy loss:\n",
    "$$ L(y_i, p_i) = - \\sum_k^K y_{ik} \\log p_{ik} $$\n",
    "where $k$ is a number of classes  \n",
    "$p_{ik}$ is probability of $k$-th class of $i$-th sample\n",
    "\n",
    "\n",
    "Decision function = probability if j class (in vector form):  \n",
    "$$ p(y_i = j | x_i) = softmax(x_i^T W)_j$$\n",
    "where $W \\in R^{KxD}$, K = number of classes, D = number of features\n",
    "$$ softmax(z)_j = \\frac {e^{z_j}} {\\sum_k e^{z_k}}$$\n",
    "\n",
    "<img src=\"images/softmax.jpg\" style=\"height:300px\">\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/multinomial.png\" style=\"height:300px\">\n",
    "\n",
    "### One-vs-rest\n",
    "Idea: build multiclass classifier from several binary classifier  \n",
    "Train K binary classifiers.  \n",
    "\n",
    "$$ \\hat y = \\arg \\max_k b_k(x) $$\n",
    "\n",
    "Notes:\n",
    "1. $b_k$ is unbalanced even if initial problem was balanced\n",
    "2. scale of the confidence values may differ between the binary classifiers $b_k$\n",
    "\n",
    "<img src=\"images/ovr.png\" style=\"height:300px\">\n",
    "\n",
    "### One-vs-one\n",
    "Idea: build multiclass classifier from several binary classifiers  \n",
    "Train $K(K-1)/2$ binary classifiers. \n",
    "\n",
    "$$ \\hat y = \\arg \\max_k \\sum_{i \\neq k} b_{ik}(x) $$\n",
    "\n",
    "Notes:\n",
    "1. One vs one is less prone to imbalance in dataset\n",
    "\n",
    "thick lines = one-vs-one  \n",
    "thin lines = one-vs-rest  \n",
    "\n",
    "<img src=\"images/1vs1.jpg\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5 SVM\n",
    "\n",
    "suppose we have some linear dicision surface\n",
    "$$f(x) = sign(<w,x>)$$\n",
    "where $<w,x>$ is a scalar product.  \n",
    "In linear kernels, $<w,x> = x^T w$   \n",
    "Then distance from point in $X^D$ to decision surface is \n",
    "$$p(x_0, f) = \\frac {|<w,x>|} {||w||}$$\n",
    "Choose scale of $w$ such that $$\\min_{x} |<w,x>| = 1$$\n",
    "Then distance from decision surface to the nearest object is\n",
    "$$\\min_{x} \\frac {|<w,x>|} {||w||} = \\frac 1 {||w||} \\min_{x} |<w,x>| = \\frac 1 {||w||}$$\n",
    "\n",
    "For linear separable case we have optimization problem:\n",
    "$$\\frac 1 2 ||w||^2 + \\sum \\rightarrow \\min_w$$\n",
    "$$ s.t. \\; y_i <w,x_i> \\;\\geq 1 $$\n",
    "\n",
    "For linear inseparable case we introduce corrections for each object $\\xi_i$:\n",
    "\n",
    "$$\\frac 1 2 ||w||^2 + C \\sum_{i=1}^N \\xi_i \\sum \\rightarrow \\min_{w, \\xi_i}$$\n",
    "$$ s.t. \\; y_i <w,x_i> \\; \\geq 1 - \\xi_i $$\n",
    "$$ s.t. \\; \\xi_i \\geq 0 $$\n",
    "\n",
    "OR:\n",
    "$$ \\xi_i = max(0, 1 - y_i <w,x_i>)$$\n",
    "\n",
    "And finally,\n",
    "\n",
    "$$\\frac 1 2 ||w||^2 + C \\sum_{i=1}^N max(0, 1 - y_i <w,x_i>) \\sum \\rightarrow \\min_w$$\n",
    "\n",
    "!!! Notice, that unlike logistic regression, weight norm penalty already build in the model.\n",
    "\n",
    "<img src=\"images/svm.png\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Kernels\n",
    "  \n",
    "* aka feature engineering  \n",
    "\n",
    "Introduce  \n",
    "$$\\phi: X \\rightarrow H$$  \n",
    "$$f(x) = <w, \\phi(x)> $$\n",
    "where H some Reproducing Kernel Hilbert Hilbert space (basically, euclidean space with scalar product)  \n",
    "\n",
    "But how to choose $\\phi$ ?\n",
    "\n",
    "* aka similarity function  \n",
    "\n",
    "Actually, you don't even need to write $\\phi$ explicitly  \n",
    "\n",
    "Def: Kernel\n",
    "$$K(x,z) = <\\phi(x), \\phi(z)> $$\n",
    "\n",
    "Th: Mercer:\n",
    "K(x,z) is a kernel function <=>\n",
    "1. K(x,z) = K(z,x)\n",
    "2. for any finite $\\{x_i\\}_{i=1}^N$ the matrix $K(x_i, x_j)$ is positive semi-define.  \n",
    "\n",
    "Th: Representer  Theorem\n",
    "\n",
    "$$ f(x) = \\sum_{i=1}^N w_i K(x, x_i) $$\n",
    "\n",
    "Pros:  \n",
    "1. using domain knowledge to construct $\\phi$ or $K$  \n",
    "\n",
    "Cons:  \n",
    "1. $O(N^2)$ complexity on memory and inference time.  \n",
    "\n",
    "**RBF kernel**\n",
    "\n",
    "$$ K(X,Y) = \\exp( - \\gamma || X - Y ||_2^2) $$\n",
    "\n",
    "<img src=\"images/rbf.png\" style=\"height:200px\">\n",
    "\n",
    "**Polynomial kernel**\n",
    "\n",
    "$$ K(X,Y) = (\\gamma <X, Y> + \\epsilon)^d $$\n",
    "\n",
    "<img src=\"images/poly.png\" style=\"height:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Evaluation \n",
    "\n",
    "### For binary classification:\n",
    "\n",
    "Accuracy $ = \\frac 1 N \\sum_{i=1}^N [\\hat y_i = y_i]$\n",
    "\n",
    "TP = true positive  \n",
    "FP = false positive  \n",
    "TN = true negative  \n",
    "FN = false negstive \n",
    "\n",
    "$$precision = \\frac {TP} {TP + FP}$$  \n",
    "$$recall = \\frac {TP}  {TP + FN}$$\n",
    "\n",
    "<img src=\"images/f1.png\" style=\"height:600px\">\n",
    "\n",
    "$$ F1 = \\frac {2 * precision * recall } {precision + recall} $$\n",
    "\n",
    "$FPR = \\frac {FP} {FP + TN} $ false negative rate    \n",
    "$TPR = \\frac {TP} {TP + FN} $ true positive rate  \n",
    "\n",
    "AUC = area under the curve ROC  \n",
    "$ROC(t) = (TPR(t), FPR(t))$ is parametrized by threshold $t$ on the probability $p(y=1|x)$ \n",
    "\n",
    "<img src=\"images/auc.png\" style=\"height:200px\">\n",
    "\n",
    "note:\n",
    "* you use hard labels for Accuracy and F1\n",
    "* you use class probabilities for AUC\n",
    "\n",
    "### For multiclass:\n",
    "\n",
    "There is no direct quality metric, it is assembled from metrics for K binary classification problems.\n",
    "\n",
    "$$macro\\_precision = \\frac 1 K \\sum_k \\frac {TP_k} {TP_k + FP_k}$$  \n",
    "$$macro\\_recall = \\frac 1 K \\sum_k \\frac {TP_k} {TP_k + FN_k}$$  \n",
    "$$ macro\\_f1 = \\frac {2 * macro\\_precision * macro\\_recall } {macro\\_precision + macro\\_recall} $$\n",
    "\n",
    "note: macro averaging is insensitive to imbalanced datasets\n",
    "\n",
    "$$micro\\_precision =  \\frac { \\sum_k TP_k} { \\sum_k TP_k + \\sum_k FP_k}$$  \n",
    "$$micro\\_recall = \\frac {\\sum_k TP_k} {\\sum_k TP_k + \\sum_k FN_k}$$  \n",
    "$$ micro\\_f1 = \\frac {2 * micro\\_precision * micro\\_recall } {micro\\_precision + micro\\_recall} $$\n",
    "\n",
    "weighted:\n",
    "\n",
    "$$weighted\\_precision = \\frac 1 K \\sum_k \\frac {|K|} {N} \\frac {TP_k} {TP_k + FP_k}$$  \n",
    "$$weighted\\_recall = \\frac 1 K \\sum_k \\frac {|K|} {N} \\frac {TP_k} {TP_k + FN_k}$$  \n",
    "$$weighted\\_f1 = \\frac {2 * weighted\\_precision * weighted\\_recall } {weighted\\_precision + weighted\\_recall} $$\n",
    "\n",
    "~ weight in proportion of class size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
