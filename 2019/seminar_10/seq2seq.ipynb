{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq\n",
    "\n",
    "\n",
    "Outline:  \n",
    "1. Neural Machine Translation\n",
    "1. RNN Seq2Seq Architecture\n",
    "1. Teacher Forcing\n",
    "1. Attention in RNN\n",
    "1. Transformer\n",
    "\n",
    "Readings:\n",
    "1. http://jalammar.github.io/illustrated-transformer/\n",
    "1. http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "1. https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/\n",
    "1. https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Neural Machine Translation\n",
    "\n",
    "Why it's hard?\n",
    "\n",
    "Let \n",
    "$x$ - source lang  \n",
    "$y$ - target lang\n",
    "\n",
    "$$ y^{*} = \\arg \\max_{y} P(y|x) = \\arg \\max_y \\prod_{t=1}^T P(y_t | y_{<t}, x)$$\n",
    "\n",
    "\n",
    "### BLEU score\n",
    "\n",
    "1. N-gram overlap between candidates and reference translations (clipped) \n",
    "1. Compute precision for n-grams of length 1 to 4\n",
    "1. Add brevity penalty if too short\n",
    "1. Compute over the whole corpus\n",
    "\n",
    "<img src=\"images/bleu.jpg\" style=\"height:500px\">\n",
    "\n",
    "\n",
    "### ROUGE score\n",
    "\n",
    "Overlap between n-grams of hypothesis and reference sentences\n",
    "\n",
    "$$ROUGE-N = \\frac {number\\_of\\_common\\_Ngrams} {total\\_number\\_of\\_Ngrams\\_in\\_reference}$$\n",
    "\n",
    "### METEOR\n",
    "\n",
    "R - unigram recall  \n",
    "P - unigram precision  \n",
    "\n",
    "```\n",
    "To take into account longer matches, METEOR computes a penalty for a given alignment as follows. First, all the unigrams in the system translation that are mapped to unigrams in the reference translation are grouped into the fewest possible number of chunks such that the unigrams in each chunk are in adjacent positions in the system translation, and are also mapped to unigrams that are in adjacent positions in the reference translation. Thus, the longer the n-grams, the fewer the chunks, and in the extreme case where the entire system translation string matches the reference translation there is only one chunk. In the other extreme, if there are no bigram or longer matches,\n",
    "there are as many chunks as there are unigram matches. \n",
    "```\n",
    "\n",
    "Penalty for alignment:\n",
    "$$p = 0.5 (\\frac c u)^3$$\n",
    "where   \n",
    "$c$ - number of chunks  \n",
    "$u$ - number of matched n-grams  \n",
    "\n",
    "$$METEOR = \\frac {10 P R} {R + 9 P} (1-p) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 RNN Seq2Seq Architecture\n",
    "\n",
    "<img src=\"images/seq2seq.png\" style=\"height:300px\">\n",
    "\n",
    "\n",
    "From source sequence generate target sequence.  \n",
    "\n",
    "\n",
    "**Encoder**:   \n",
    "$h_t = LSTM(h_{t-1}, x_t)$ - encoder hidden state     \n",
    "$e_t = out_e(h_t)$ - output of encoder at time $t$     \n",
    "\n",
    "**Decoder**:  \n",
    "$s_t = LSTM(s_{t-1}, y_{t-1})$ - decoder hidden state  \n",
    "$g_t = out_g(s_t)$ - output of decoder at time $t$  \n",
    "$p_t = softmax(g_t)$ - probabilities of tokens at time $t$  \n",
    "$y_t = argmax(p_t)$ - predicted token at time $t$  \n",
    "$s_o = h_T$ - initial decoder hidden state is the last encoder hidden state.  \n",
    "\n",
    "\n",
    "Seq2seq is a classification task. Because at every time step you have to choose what token to output.   \n",
    "Loss function is very similar to autoregression case = is an average of cross-entropy loss on tokens. \n",
    "Since it is an average, it doesn't depend on the sequence length. \n",
    "\n",
    "$$ Loss(S_{pred}, S_{target}) = \\frac 1 {|S_{target}|} \\sum_{i=1}^{|S_{target}|} cross\\_entropy(y_i, \\hat y_i)$$\n",
    "where  \n",
    "$S_{pred}, S_{target}$ - predicted and target sequences.  \n",
    "$y_i, \\hat y_i$ - target token and predicted token for corresponding sequences.  \n",
    "If predicted sequence is shorter than the target sequence, it should be padded to target sequence length.  \n",
    "If predicted sequence is longer than the target sequence, it should be cutted to target sequence length.    \n",
    "Because you train your model with mini-batches, you have to pad your target sequences to have common length.  \n",
    "Padding value should not be counted as an error.  \n",
    "All that is done for your by `torch.nn.CrossEntropyLoss(ignore_index = <your padding value>)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training/Inference Discrepancy. Teacher forcing \n",
    "\n",
    "There is a problem with training the decoder.  \n",
    "Because at $y_{t}$ depends on $y_{t-1}, ..., y_0$, if at some timestamp a wrong token is predicted, the rest of the sequence will be wrong too.   \n",
    "So teacher forcing method was introduced.  \n",
    "\n",
    "<img src=\"images/teacher.png\" style=\"height:400px\">\n",
    "\n",
    "At the training phase, at every timestep you give the decoder **true previous token $x_{t-1}$** to predict the current one $y_t$.  \n",
    "At the inference phase, at every timestep you give the decoder **predicted previous token $y_{t-1}$** to predict the current one $y_t$.\n",
    "\n",
    "Well, there is another problem with teacher forcing: the model is tought to do something different from it actually should do. Conditioning on the true previous token is a bit easier.  \n",
    "Though, in pytorch you can mix both regimes: some batches train with teacher forcing and others - with vanilla BPTT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with Vanilla Encoder-Decoder Architecture\n",
    "\n",
    "1. Poor performance on long sentences\n",
    "1. Bias towards shorter candidates\n",
    "1. Fluent but inadequate output\n",
    "1. No guarantee that all input words are translated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Attention\n",
    "\n",
    "<img src=\"images/attention.jpg\" style=\"height:500px\">\n",
    "\n",
    "Attention is a mechanism of conditioning of every output on a weighted sum of source inputs.\n",
    "\n",
    "Introduce attention through new function $f$:  \n",
    "$ \\alpha_{t'} = f(g_{t-1}, e_{t'}) $ - weights of source tokens.    \n",
    "$ \\bar \\alpha = softmax(\\alpha) $ - normalize weights.  \n",
    "$ c_t  = \\sum_{t'=0}^T \\bar \\alpha_{t'} e_{t'}$ - weighted sum\n",
    "\n",
    "**Encoder**:   \n",
    "$h_t = LSTM(h_{t-1}, x_t)$ - encoder hidden state     \n",
    "$e_t = out_e(h_t)$ - output of encoder at time $t$     \n",
    "\n",
    "**Decoder**:  \n",
    "$s_t = LSTM(s_{t-1}, [y_{t-1}, c_t])$ - decoder hidden state  \n",
    "$g_t = out_g(s_t)$ - output of decoder at time $t$  \n",
    "$p_t = softmax(g_t)$ - probabilities of tokens at time $t$  \n",
    "$y_t = argmax(p_t)$ - predicted token at time $t$  \n",
    "$s_o = h_T$ - initial decoder hidden state is the last encoder hidden state.  \n",
    "\n",
    "\n",
    "Usual choices for attention functions:\n",
    "\n",
    "$f(h, e) = h^T e$ - dot  \n",
    "$f(h, e) = h^T W e$ - general    \n",
    "$f(h, e) = v^T tanh(W [h, e])$  concat  \n",
    "\n",
    "\n",
    "Basic attention mechanisms:  \n",
    "\n",
    "<img src=\"images/attn2.png\" style=\"height:500px\">\n",
    "\n",
    "\n",
    "\n",
    "Bonus: interpretable models - because every predicted token is conditioned on a weighted sum of input tokens, it means, that you can see wich tokens were most infuentional for the prediced one.    \n",
    "\n",
    "\n",
    "<img src=\"images/viz.png\" style=\"height:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "### Self Attention\n",
    "\n",
    "<img src=\"images/selfatt.png\" style=\"height:300px\">\n",
    "\n",
    "### Positional encoding\n",
    "\n",
    "```\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "```\n",
    "\n",
    "<img src=\"images/pos.png\" style=\"height:300px\">\n",
    "\n",
    "### Layer Norm\n",
    "\n",
    "BatchNorm:\n",
    "\n",
    "$$\\mu_j = \\frac 1 N \\sum_{i=1}^N x_{ij}$$\n",
    "$$\\sigma^2_j = \\frac 1 N \\sum_{i=1}^N (x_{ij} - \\mu_j)^2$$\n",
    "$$\\hat x_{ij} = \\frac {x_{ij} - \\mu_j} {\\sqrt{\\sigma^2_j + \\epsilon}}$$\n",
    "$$\\hat x_{ij} = \\alpha \\hat x_{ij}  + \\beta$$\n",
    "\n",
    "\n",
    "Problems with regular BatchNorm:\n",
    "1. It puts a lower limit on the batch size\n",
    "1. It makes batch normalization difficult to apply to recurrent connections in recurrent neural network\n",
    "```\n",
    "In a recurrent neural network, the recurrent activations of each time-step will have different statistics. This means that we have to fit a separate batch normalization layer for each time-step. This makes the model more complicated and – more importantly – it forces us to store the statistics for each time-step during training.\n",
    "```\n",
    "\n",
    "LayerNorm:\n",
    "\n",
    "$$\\mu_i = \\frac 1 C \\sum_{j=1}^C x_{ij}$$\n",
    "$$\\sigma^2_i = \\frac 1 C \\sum_{i=1}^C (x_{ij} - \\mu_i)^2$$\n",
    "$$\\hat x_{ij} = \\frac {x_{ij} - \\mu_i} {\\sqrt{\\sigma^2_i + \\epsilon}}$$\n",
    "$$\\hat x_{ij} = \\alpha \\hat x_{ij}  + \\beta$$\n",
    "\n",
    "<img src=\"images/layer.png\" style=\"height:200px\">\n",
    "\n",
    "### General Architecture\n",
    "\n",
    "<img src=\"images/tr.png\" style=\"height:600px\">\n",
    "\n",
    "\n",
    "### Masking target\n",
    "\n",
    "<img src=\"images/mask.png\" style=\"height:200px\">\n",
    "\n",
    "### Problems with NMT\n",
    "\n",
    "1. Number of parameters dominated by size of word embeddings => shared embeddings \n",
    "1. Unknown word rate depends on vocab size\n",
    "1. UNK replacement (using alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
