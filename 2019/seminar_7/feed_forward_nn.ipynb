{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Neural Networks\n",
    "\n",
    "## Reading\n",
    "1. Goodfellow. Deep Learning\n",
    "1. https://pytorch.org/tutorials/\n",
    "1. <del>How to Train Your Dragon</del>. https://myrtle.ai/how-to-train-your-resnet/\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. General architecture\n",
    "1. Neuron\n",
    "1. Layers\n",
    "1. Training\n",
    "1. Activation functions\n",
    "1. Weight Initialization\n",
    "1. Optimization\n",
    "1. Regularization\n",
    "1. Universal Approximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 General Architecture\n",
    "\n",
    "NN as a composition of functions\n",
    "\n",
    "$$ F(x) = f_{w_n} \\circ f_{w_{n-1}} \\circ .. f_{w_1} (x) $$\n",
    "\n",
    "<img src=\"images/ff.jpg\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Neuron\n",
    "\n",
    "$$ y = \\sum_{i=1}^N f(w_ji x_i + b)$$\n",
    "\n",
    "where  \n",
    "$f$ - some non-linear **activation** function  \n",
    "$w_i$ - learnable weights   \n",
    "$b$ - learnable bias - usually incorporated into X     \n",
    "$y$ - output of neuron  \n",
    "$x$ - input of neuron  \n",
    "\n",
    "<img src=\"images/neuron.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Dense Layers\n",
    "\n",
    "It's more convinient to express the same thing in vector form  \n",
    "\n",
    "$$Y = f(XW)$$\n",
    "\n",
    "where  \n",
    "$X \\in R^{NxD_1}$ - input of layer  \n",
    "$Y \\in R^{NxD_2}$ - output of layer  \n",
    "$W \\in R^{D_1 x D_2} $ - learnable weight matrix\n",
    "\n",
    "\n",
    "<img src=\"images/weights.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Training \n",
    "\n",
    "1. forward pass => compute loss \n",
    "1. backward pass => compute gradients on learnable weights\n",
    "1. update weights according to your optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Activation fuctions\n",
    "\n",
    "Why we do not use linear activations?\n",
    "\n",
    "Activation functions supposed to be nice in the sense of gradient properties.\n",
    "\n",
    "### Sigmoid \n",
    "\n",
    "$$\\sigma(z) = \\frac 1 {1 + \\exp^{-z}}$$\n",
    "\n",
    "* vanishing gradient\n",
    "* bad output distribution\n",
    "\n",
    "<img src=\"images/sigmoid.png\" style=\"height:300px\">\n",
    "\n",
    "\n",
    "### Tanh\n",
    "\n",
    "* vanishing gradient\n",
    "\n",
    "<img src=\"images/tanh.jpg\" style=\"height:300px\">\n",
    "\n",
    "### RELU\n",
    "\n",
    "$$RELU(z) = \\max(0, z)$$\n",
    "\n",
    "* dead neurons if $z < 0$\n",
    "\n",
    "<img src=\"images/sigmoid.png\" style=\"height:300px\">\n",
    "\n",
    "### ELU\n",
    "\n",
    "$ELU(z) = z$ if $z > 0$  \n",
    "$ELU(z) = \\alpha (\\exp^z-1)$ if $z \\leq 0 $  \n",
    "\n",
    "* little longer computation than RELU\n",
    "\n",
    "<img src=\"images/elu.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Weight initialization\n",
    "\n",
    "As we train out neural network with SGD, it is important to have good initial point to start.  \n",
    "Usually use use:\n",
    "1. uniform distribution in [-1,1]   \n",
    "1. standart normal distribution  $N(0,1)$\n",
    "1. xavier distribution (discuss later, in conv networks)  \n",
    "\n",
    "\n",
    "* Why we use distributions centered around zero?  \n",
    "* How it is connected with activation functions?  \n",
    "* shared weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Optimization\n",
    "\n",
    "\n",
    "### Vanilla SGD\n",
    "\n",
    "$\\theta_0 \\leftarrow $ init\n",
    "\n",
    "for random batch on step $t$ = 1..max_iter:\n",
    "\n",
    "$$ \\theta_t = \\theta_{t-1} - \\alpha \\nabla_{\\theta} J(\\theta_{t-1}) $$\n",
    "\n",
    "$J$ - loss function  \n",
    "$\\theta_t$ - learnable parameters at step $t$  \n",
    "$\\alpha$ - learning rate  \n",
    "\n",
    "* good theoretic properties\n",
    "* slow convergence\n",
    "\n",
    "<img src=\"images/sgd.gif\" style=\"height:300px\">\n",
    "\n",
    "\n",
    "### SGD with momentum\n",
    "\n",
    "$\\theta_0 \\leftarrow $ init  \n",
    "$ m_0 \\leftarrow  0$   \n",
    "\n",
    "for random batch on step $t$ = 1..max_iter:\n",
    "\n",
    "$$ m_t = \\beta m_{t-1} + (1 - \\beta)\\nabla_{\\theta} J(\\theta_{t-1})$$\n",
    "$$ \\theta_t = \\theta_{t-1} - \\alpha m_t $$\n",
    "\n",
    "where  \n",
    "$m_t$ - accumulated gradient at step $t$   \n",
    "$\\beta$ - momentum parameter  \n",
    "\n",
    "\n",
    "* Momentum cancels moves in \"random\" directions from stochastic nature of SGD\n",
    "* Momentum ~ inertia\n",
    "\n",
    "<img src=\"images/momentum.png\" style=\"height:150px\">\n",
    "\n",
    "\n",
    "### RmsProp\n",
    "\n",
    "$\\theta_0 \\leftarrow $ init  \n",
    "$ v_0 \\leftarrow  0$   \n",
    "\n",
    "for random batch on step $t$ = 1..max_iter:\n",
    "\n",
    "$$ g_t = \\nabla_{\\theta} J(\\theta_{t-1}) $$\n",
    "$$  v_t = \\beta v_{t-1} + (1-\\beta) g^2_t $$\n",
    "$$  \\theta_t = \\theta_{t-1} - \\frac {\\alpha} {\\sqrt {v_t} + \\epsilon} g_t $$\n",
    "\n",
    "where  \n",
    "$v_t$ - accumulated squared components of gradient\n",
    "$\\beta$ - parameter  \n",
    "$\\epsilon << 1$ - to prevent division by zero \n",
    "\n",
    "\n",
    "* gradient direction carries more information than its norm  \n",
    "* adjust gradient step size \n",
    "\n",
    "\n",
    "###  Adam\n",
    "\n",
    "$\\theta_0 \\leftarrow $ init  \n",
    "$ v_0 \\leftarrow  0$ \n",
    "$ m_0 \\leftarrow  0$ \n",
    "\n",
    "for random batch on step $t$ = 1..max_iter:\n",
    "\n",
    "\n",
    "$$ g_t = \\nabla_{\\theta} J(\\theta_{t-1}) $$\n",
    "\n",
    "$$ m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t $$\n",
    "$$ v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t $$\n",
    "$$ \\hat m_t = \\frac {m_t} {1 - \\beta_1^t} $$\n",
    "$$ \\hat v_t = \\frac {v_t} {1 - \\beta_2^t}  $$\n",
    "$$  \\theta_t = \\theta_{t-1} - \\frac {\\alpha} {\\sqrt {\\hat v_t} + \\epsilon} \\hat m_t $$\n",
    "\n",
    "where  \n",
    "$m_t$ - accumulated momentum\n",
    "$v_t$ - accumulated squared components of gradient\n",
    "$\\beta_1, \\beta_2$ - parameters  \n",
    "$\\epsilon << 1$ - to prevent division by zero \n",
    "\n",
    "\n",
    "* essentially SGD with momentum + RmsProp\n",
    "* corrections for $\\hat m_t, \\hat v_t$ are to make first optimization steps more stable. Because the calculation of $ m_t, v_t$ can be seen as geometric series\n",
    "\n",
    "\n",
    "<img src=\"images/comparison.gif\" style=\"height:400px\">\n",
    "\n",
    "### Learning schedule\n",
    "\n",
    "How to adjust gradient step size?\n",
    "\n",
    "1. Reduce on plato by some factor  \n",
    "1. Reduce on each iteration  \n",
    "....  \n",
    "find more variants in torch.optim.lr_schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Regularization in neural networks\n",
    "\n",
    "Most popular:  \n",
    "1. $L_2$ norm regularization through weight decay\n",
    "1. Early stopping \n",
    "1. Data augmentation. Create new samples from the same domain to increase size of your dataset. Remember generalization bounds.  \n",
    "1. Dropout. Drop random nodes in a layer with probability $p$    \n",
    "1. Batch Normalization\n",
    "\n",
    "\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Theere are 2 interpretations for dropout:  \n",
    "1. \"Bagging\" over neural networks  \n",
    "1. Avoid feature coadaptation  \n",
    "\n",
    "Difference between bagging and dropout:\n",
    "\n",
    "$$ p(y|x) = \\frac 1 K \\sum_{i=1}^K p_i(y|x)$$ for bagging  \n",
    "\n",
    "$$ p(y|x) = \\sum_{\\mu} p(\\mu) p(y | x, \\mu)$$ for dropout, where $\\mu$ is mask on weights. There is an exponential number of masks for fixed number of weights, that makes dropout more effective than explicit bagging.  \n",
    "\n",
    "\n",
    "**On training**:  \n",
    "On each batch randomly remove neurons in the previous layer with probability $p$.\n",
    "\n",
    "**On inference**:  \n",
    "\n",
    "Ideally, sample all $2^n$ dropped-out networks and average predictions.  \n",
    "In practice, approximate by using the full network with each node's output weighted by a factor of $1-p$, so the expected value of the output of any node is the same as in the training stages.   \n",
    "=> Although it effectively generates 2^{n} neural nets, but at test time only a single network needs to be tested.\n",
    "\n",
    "\n",
    "<img src=\"images/drop.png\" style=\"height:300px\">\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "**On training**:  \n",
    "\n",
    "on every batch $t$:\n",
    "\n",
    "$$ \\mu_t = \\frac 1 m \\sum_{i=1}^m x_i $$\n",
    "$$ \\sigma^2_t = \\frac 1 m \\sum_{i=1}^m (x_i - \\mu_t) $$\n",
    "$$ \\hat x_i = \\frac {x_i - \\mu_t} {\\sigma_t + \\epsilon} $$\n",
    "$$ y_i = \\gamma \\hat x_i + \\beta = BN_{\\gamma, \\beta}(x_i) $$\n",
    "\n",
    "where  \n",
    "$\\mu_t$ - estimated batch mean  \n",
    "$\\sigma^2_t$ - estimated batch variance  \n",
    "$\\hat x_i$ - normalized input  \n",
    "$\\gamma$ - learnable scale parameter  \n",
    "$\\beta$ - learnable shift parameter  \n",
    "\n",
    "**On inference** we can't compute $\\mu_t, \\sigma^2_t$. Instead, we use some running average over $\\mu_t, \\sigma^2_t$ that were observed during training.\n",
    "\n",
    "* NN in theory can learn $\\gamma, \\beta$ to undo batch normalization. In practice, they usually don't\n",
    "* BatchNorm stabilizes training by making surface of loss function more smooth\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/batchnorm.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Appoximators\n",
    "\n",
    "Universal Appoximation theorem\n",
    "\"Feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $R^n$, under mild assumptions on the activation function\"\n",
    "\n",
    "### Shallow networks\n",
    "\n",
    "Though we can approximate any function with just one hidden layer, it need needs exponential number of nodes in a layer\n",
    "\n",
    "### Feature representation\n",
    "\n",
    "In practice, features learned on deeper layers correspond to more abstract object attributes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demo1, MNIST\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Demo1(nn.Module):\n",
    "    \n",
    "    def __init__(self, p):\n",
    "        super(Demo1, self).__init__()\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.fc1 = nn.Linear(28*28, 100)\n",
    "        self.fc2 = nn.Linear(100,100)\n",
    "        self.bn = nn.BatchNorm1d(100)\n",
    "        \n",
    "        # out dim = 10 because we have only 10 digits\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # 1st layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc2(x)\n",
    "        # BatchNorm\n",
    "        x = self.bn(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        # dropout layer\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        # 2nd layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(model, train_loader, val_loader, n_batches, optimizer, criterion, n_epochs=20, \n",
    "             device=torch.device('cpu'),\n",
    "             mu=0.9, \n",
    "             logdir=None,\n",
    "             checkdir=None,\n",
    "             reduce_lr_patience=2,\n",
    "             verbose=True\n",
    "            ):\n",
    "    if logdir:\n",
    "        sw = SummaryWriter(logdir)\n",
    "    else:\n",
    "        sw = None\n",
    "        \n",
    "    history = []\n",
    "    \n",
    "    if reduce_lr_patience > 0:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=reduce_lr_patience, verbose=verbose)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # training mode\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        \n",
    "        if verbose:\n",
    "            batch_iter = tqdm_notebook(enumerate(train_loader), total=n_batches, desc='epoch %d' % (epoch + 1), leave=True)\n",
    "        else:\n",
    "            batch_iter = enumerate(train_loader)\n",
    "            \n",
    "        for i, (X, y) in batch_iter:\n",
    "            \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            prediction = model(X)\n",
    "            \n",
    "            loss = criterion(prediction, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            current_loss = loss.data.detach().item()\n",
    "            # smooth loss\n",
    "            running_loss = running_loss * mu + current_loss * (1-mu)\n",
    "            \n",
    "            if verbose:\n",
    "                batch_iter.set_postfix(loss='%.4f' % running_loss)\n",
    "                \n",
    "            niter = epoch * n_batches + i\n",
    "            \n",
    "            if sw:\n",
    "                sw.add_scalar('Train/Loss', current_loss, niter)\n",
    "                \n",
    "                \n",
    "        # validation mode \n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                prediction = model(X)\n",
    "                loss = criterion(prediction, y)\n",
    "                loss = loss.data.detach().item()\n",
    "                val_loss.append( loss )\n",
    "\n",
    "        val_loss = np.mean(val_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print('validation loss=%.4f' % val_loss)\n",
    "\n",
    "        if sw:\n",
    "            sw.add_scalar('Validation/Loss', val_loss, epoch)\n",
    "\n",
    "        if reduce_lr_patience > 0:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        if checkdir:\n",
    "            torch.save(model.state_dict(), checkdir + '/epoch_%d_val_loss_%f' % (epoch, val_loss))\n",
    "\n",
    "\n",
    "        history.append({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': running_loss,\n",
    "            'val_loss': val_loss,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    datasets.MNIST('./', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    datasets.MNIST('./', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12c52d310>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = train_loader.dataset[0][0]\n",
    "sample = np.squeeze(sample)\n",
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.1\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = Demo1(dropout_rate).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_batches = int(np.ceil(len(train_loader.dataset) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf log_demo1\n",
    "!rm -rf check_demo1\n",
    "!mkdir check_demo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denaas/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802444bab2d044eb9fe8e712559fe81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=469, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss=0.2200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68147ffcde94608873350492fad51fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 2', max=469, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss=0.1506\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb56be7515ac4bbf88f48e69ce8ab9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 3', max=469, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss=0.1188\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2573ba38f1e94a9daeb02b4b232d4710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 4', max=469, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss=0.1053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521fa06a94d94e89b474784ff1178b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 5', max=469, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss=0.0918\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8549890927a54054a3acc8e7d9b3061b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 6', max=469, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss=0.0881\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f497a9256744fc80ed3c7748e8cb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 7', max=469, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss=0.0838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a54d3d3c0124592806173c25ed5e971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 8', max=469, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss=0.0798\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1e7a03a0654f35b9c9185bb022eed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 9', max=469, style=ProgressStyle(description_width='ini…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss=0.0755\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20dd2cb6e354cf6a62f379971c130bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 10', max=469, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation loss=0.0746\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.266547</td>\n",
       "      <td>0.219966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.181055</td>\n",
       "      <td>0.150640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.139798</td>\n",
       "      <td>0.118827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.112262</td>\n",
       "      <td>0.105270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.092133</td>\n",
       "      <td>0.091781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.086446</td>\n",
       "      <td>0.088132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.079673</td>\n",
       "      <td>0.083788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.070641</td>\n",
       "      <td>0.079781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.064878</td>\n",
       "      <td>0.075453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.064865</td>\n",
       "      <td>0.074618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  val_loss\n",
       "0      0    0.266547  0.219966\n",
       "1      1    0.181055  0.150640\n",
       "2      2    0.139798  0.118827\n",
       "3      3    0.112262  0.105270\n",
       "4      4    0.092133  0.091781\n",
       "5      5    0.086446  0.088132\n",
       "6      6    0.079673  0.083788\n",
       "7      7    0.070641  0.079781\n",
       "8      8    0.064878  0.075453\n",
       "9      9    0.064865  0.074618"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn(model, train_loader, val_loader, n_batches, optimizer, criterion, \n",
    "         n_epochs=10,\n",
    "         device=device,\n",
    "         logdir='log_demo1',\n",
    "         checkdir='check_demo1',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
